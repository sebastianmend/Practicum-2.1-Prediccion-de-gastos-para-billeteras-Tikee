{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Cargar el dataset desde el archivo JSON\u001b[39;00m\n\u001b[0;32m      5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/transacciones_actualizadas.json\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Corrige el nombre del archivo\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Añadido lines=True si el JSON tiene múltiples objetos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Verificar si el dataset tiene datos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[1;32mc:\\Practicum2.1\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:199\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Practicum2.1\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:299\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting with Pandas version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m all arguments of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00marguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be keyword-only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n\u001b[0;32m    298\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel)\n\u001b[1;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Practicum2.1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:563\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m json_reader:\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Practicum2.1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:692\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m    691\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 692\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    694\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Practicum2.1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:716\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    714\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 716\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Practicum2.1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:831\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_numpy()\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 831\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_no_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Practicum2.1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:1079\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1075\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[1;32m-> 1079\u001b[0m         \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     )\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1082\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1083\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1084\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1085\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# Cargar el dataset desde el archivo JSON\n",
    "file_path = \"dataset/transacciones_actualizadas.json\"  # Corrige el nombre del archivo\n",
    "df = pd.read_json(file_path)  # Añadido lines=True si el JSON tiene múltiples objetos\n",
    "\n",
    "# Verificar si el dataset tiene datos\n",
    "if df.empty:\n",
    "    print(\"El dataset está vacío. Verifica el archivo JSON.\")\n",
    "else:\n",
    "    # Crear un mapa centrado en las coordenadas promedio del dataset\n",
    "    centro_latitud = df[\"latitud\"].mean()\n",
    "    centro_longitud = df[\"longitud\"].mean()\n",
    "    mapa = folium.Map(location=[centro_latitud, centro_longitud], zoom_start=12)\n",
    "\n",
    "    # Añadir marcadores para cada transacción en el dataset\n",
    "    for _, fila in df.iterrows():\n",
    "        # Verificar que las coordenadas sean válidas\n",
    "        if pd.notnull(fila[\"latitud\"]) and pd.notnull(fila[\"longitud\"]):\n",
    "            popup_info = f\"\"\"\n",
    "            <b>ID Transacción:</b> {fila['id_transaccion']}<br>\n",
    "            <b>Monto:</b> {fila['monto']}<br>\n",
    "            <b>Ordenante:</b> {fila['ordenante_nombre_completo']}<br>\n",
    "            <b>Beneficiario:</b> {fila['beneficiario_nombre_completo']}\n",
    "            \"\"\"\n",
    "\n",
    "            folium.Marker(\n",
    "                location=[fila[\"latitud\"], fila[\"longitud\"]],\n",
    "                popup=folium.Popup(popup_info, max_width=300),\n",
    "                icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n",
    "            ).add_to(mapa)\n",
    "\n",
    "    # Guardar el mapa en un archivo HTML\n",
    "    output_path = \"mapa_transacciones.html\"\n",
    "    mapa.save(output_path)\n",
    "    print(f\"Mapa interactivo guardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados exitosamente.\n",
      "Mapa interactivo guardado en: mapa_transacciones.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# Ruta al archivo JSON\n",
    "file_path = \"dataset/transacciones_actualizadas.json\"\n",
    "\n",
    "# Cargar el JSON como lista\n",
    "try:\n",
    "    df = pd.read_json(file_path)  # No usar `lines=True`\n",
    "    print(\"Datos cargados exitosamente.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error al cargar el archivo JSON: {e}\")\n",
    "    raise\n",
    "\n",
    "# Verificar si el dataset tiene datos\n",
    "if df.empty:\n",
    "    raise ValueError(\"El archivo JSON no contiene datos o está vacío.\")\n",
    "\n",
    "# Crear un mapa centrado en las coordenadas promedio del dataset\n",
    "centro_latitud = df[\"latitud\"].mean()\n",
    "centro_longitud = df[\"longitud\"].mean()\n",
    "mapa = folium.Map(location=[centro_latitud, centro_longitud], zoom_start=12)\n",
    "\n",
    "# Añadir marcadores para cada transacción en el dataset\n",
    "for _, fila in df.iterrows():\n",
    "    popup_info = f\"<b>ID Transacción:</b> {fila['id_transaccion']}<br>\" \\\n",
    "                 f\"<b>Monto:</b> {fila['monto']}<br>\" \\\n",
    "                 f\"<b>Ordenante:</b> {fila['ordenante_nombre_completo']}<br>\" \\\n",
    "                 f\"<b>Beneficiario:</b> {fila['beneficiario_nombre_completo']}\"\n",
    "\n",
    "    folium.Marker(\n",
    "        location=[fila[\"latitud\"], fila[\"longitud\"]],\n",
    "        popup=folium.Popup(popup_info, max_width=300),\n",
    "        icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n",
    "    ).add_to(mapa)\n",
    "\n",
    "# Guardar el mapa en un archivo HTML\n",
    "output_path = \"mapa_transacciones.html\"\n",
    "mapa.save(output_path)\n",
    "print(f\"Mapa interactivo guardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles en el dataset:\n",
      "Index(['id_transaccion', 'fecha_hora', 'monto', 'latitud', 'longitud',\n",
      "       'dispositivo_id', 'ordenante_nombre_completo',\n",
      "       'ordenante_numero_cuenta', 'ordenante_cedula',\n",
      "       'ordenante_institucion_financiera', 'ordenante_codigo_institucion',\n",
      "       'beneficiario_nombre_completo', 'beneficiario_numero_cuenta',\n",
      "       'beneficiario_cedula', 'beneficiario_institucion_financiera',\n",
      "       'beneficiario_codigo_institucion', 'direccion_ip', 'sistema_operativo',\n",
      "       'categoria', 'motivo', 'local_nombre'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# Cargar el dataset desde un archivo JSON\n",
    "file_path = r\"dataset\\transacciones_actualizadas.json\"\n",
    "\n",
    "# Cargar y verificar las columnas del dataset\n",
    "df = pd.read_json(file_path)\n",
    "print(\"Columnas disponibles en el dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Definir las columnas relevantes\n",
    "variables_interes = [\n",
    "    \"ordenante_cedula\",  # Identificador único del usuario\n",
    "    \"monto\",\n",
    "    \"fecha_hora\",\n",
    "    \"local_nombre\",\n",
    "    \"latitud\",\n",
    "    \"longitud\"\n",
    "]\n",
    "\n",
    "# Asegurarse de que las columnas relevantes estén en el dataset\n",
    "for columna in variables_interes:\n",
    "    if columna not in df.columns:\n",
    "        raise KeyError(f\"La columna '{columna}' no se encuentra en el dataset.\")\n",
    "\n",
    "# Filtrar las columnas relevantes\n",
    "df = df[variables_interes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset transformado:\n",
      "   ordenante_cedula     monto      local_nombre   latitud  longitud      hora  \\\n",
      "0        3597400581  0.977787       Desconocido  0.000018  0.999916  0.086957   \n",
      "1        3597400581  0.769383       Desconocido  0.000004  0.999988  0.000000   \n",
      "2        3597400581  0.401947       Desconocido  0.000007  0.999909  0.652174   \n",
      "3        3597400581  0.602442  Bogati Heladeria  0.999850  0.002208  0.304348   \n",
      "4        3597400581  0.958983       Magic Retro  0.998453  0.002660  0.782609   \n",
      "\n",
      "   dia_semana  local_nombre_encoded  \n",
      "0    1.000000                     3  \n",
      "1    0.333333                     3  \n",
      "2    0.500000                     3  \n",
      "3    0.500000                     0  \n",
      "4    0.500000                     7  \n"
     ]
    }
   ],
   "source": [
    "# Reemplazar valores nulos en la columna 'local_nombre' por \"Desconocido\"\n",
    "df[\"local_nombre\"] = df[\"local_nombre\"].fillna(\"Desconocido\")\n",
    "\n",
    "# Extraer la hora y el día de la semana de la columna 'fecha_hora'\n",
    "df[\"hora\"] = pd.to_datetime(df[\"fecha_hora\"]).dt.hour\n",
    "df[\"dia_semana\"] = pd.to_datetime(df[\"fecha_hora\"]).dt.weekday\n",
    "\n",
    "# Codificar la variable 'local_nombre' con LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"local_nombre_encoded\"] = label_encoder.fit_transform(df[\"local_nombre\"])\n",
    "\n",
    "# Normalizar las variables numéricas (monto, latitud, longitud, hora, día de la semana)\n",
    "scaler = MinMaxScaler()\n",
    "df[[\"monto\", \"latitud\", \"longitud\", \"hora\", \"dia_semana\"]] = scaler.fit_transform(\n",
    "    df[[\"monto\", \"latitud\", \"longitud\", \"hora\", \"dia_semana\"]]\n",
    ")\n",
    "\n",
    "# Eliminar la columna original 'fecha_hora' ya que ahora está representada como 'hora' y 'dia_semana'\n",
    "df = df.drop(columns=[\"fecha_hora\"])\n",
    "\n",
    "# Mostrar las primeras filas del dataset transformado\n",
    "print(\"Dataset transformado:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset transformado guardado en: dataset/transacciones_transformadas.json\n"
     ]
    }
   ],
   "source": [
    "# Guardar el dataset transformado en un nuevo archivo JSON\n",
    "output_path = \"dataset/transacciones_transformadas.json\"\n",
    "df.to_json(output_path, orient=\"records\", lines=True)\n",
    "print(f\"Dataset transformado guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir el dataset para entrenamiento y para testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de entrenamiento guardado en: dataset/transacciones_train.json\n",
      "Dataset de testeo guardado en: dataset/transacciones_test.json\n",
      "Dataset de entrenamiento:\n",
      "     ordenante_cedula     monto      local_nombre   latitud  longitud  \\\n",
      "100        4557997274  0.627774  Bogati Heladeria  0.999850  0.002208   \n",
      "101        4557997274  0.055855       Desconocido  0.000024  0.999990   \n",
      "102        4557997274  0.184097       Desconocido  0.000008  0.999982   \n",
      "103        4557997274  0.270921       Desconocido  0.000007  0.999911   \n",
      "104        4557997274  0.191382       Desconocido  0.000012  0.999970   \n",
      "\n",
      "         hora  dia_semana  local_nombre_encoded  \n",
      "100  0.434783    0.166667                     0  \n",
      "101  0.043478    0.166667                     3  \n",
      "102  0.391304    0.000000                     3  \n",
      "103  0.913043    0.833333                     3  \n",
      "104  0.173913    0.000000                     3  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Cargar el dataset limpio\n",
    "file_path = \"dataset/transacciones_transformadas.json\"\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Dividir los datos por usuario (ordenante_cedula)\n",
    "usuarios = df[\"ordenante_cedula\"].unique()\n",
    "\n",
    "# Crear un conjunto de usuarios para entrenamiento y testeo\n",
    "usuarios_train, usuarios_test = train_test_split(\n",
    "    usuarios, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Separar el dataset en base a los usuarios seleccionados\n",
    "df_train = df[df[\"ordenante_cedula\"].isin(usuarios_train)]\n",
    "df_test = df[df[\"ordenante_cedula\"].isin(usuarios_test)]\n",
    "\n",
    "# Guardar los datasets de entrenamiento y testeo en archivos separados\n",
    "train_path = \"dataset/transacciones_train.json\"\n",
    "test_path = \"dataset/transacciones_test.json\"\n",
    "\n",
    "df_train.to_json(train_path, orient=\"records\", lines=True)\n",
    "df_test.to_json(test_path, orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"Dataset de entrenamiento guardado en: {train_path}\")\n",
    "print(f\"Dataset de testeo guardado en: {test_path}\")\n",
    "print(\"Dataset de entrenamiento:\")\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de Modelo KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comercios más cercanos basados en el historial del usuario (desnormalizados):\n",
      "             local_nombre   latitud  longitud      hora  dia_semana\n",
      "406        Chifa Oriental  0.999268  0.001853  0.956522         1.0\n",
      "405        Chifa Oriental  0.999268  0.001853  0.956522         1.0\n",
      "409  Manantial restaurant  0.999268  0.001853  0.956522         1.0\n",
      "403        Chifa Oriental  0.999268  0.001853  0.956522         1.0\n",
      "407        Chifa Oriental  0.999268  0.001853  0.956522         1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Practicum2.1\\venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Practicum2.1\\venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Cargar el dataset\n",
    "file_path = \"dataset/transacciones_train.json\"\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# 2. Definir los pesos para las variables\n",
    "pesos = {\n",
    "    \"latitud\": 0.25,\n",
    "    \"longitud\": 0.25,\n",
    "    \"hora\": 0.25,\n",
    "    \"dia_semana\": 0.25\n",
    "}\n",
    "\n",
    "# 3. Normalizar las variables relevantes\n",
    "scaler = MinMaxScaler()\n",
    "variables_a_normalizar = [\"latitud\", \"longitud\", \"hora\", \"dia_semana\"]\n",
    "df_normalizado = df.copy()\n",
    "df_normalizado[variables_a_normalizar] = scaler.fit_transform(df[variables_a_normalizar])\n",
    "\n",
    "# Aplicar los pesos después de la normalización\n",
    "for columna, peso in pesos.items():\n",
    "    df_normalizado[columna] *= peso\n",
    "\n",
    "# 4. Características y modelo KNN\n",
    "X_recomendador = df_normalizado[[\"latitud\", \"longitud\", \"hora\", \"dia_semana\"]]\n",
    "recomendador = NearestNeighbors(n_neighbors=5, metric='euclidean')\n",
    "recomendador.fit(X_recomendador)\n",
    "\n",
    "# 5. Función para recomendar comercios\n",
    "def recomendar_por_cedula(cedula, cliente):\n",
    "    # Filtrar el historial del usuario por cédula\n",
    "    historial_usuario = df[df[\"ordenante_cedula\"] == cedula]\n",
    "    \n",
    "    if historial_usuario.empty:\n",
    "        print(f\"No se encontró historial para el usuario con cédula: {cedula}\")\n",
    "        return None\n",
    "\n",
    "    # Normalizar los datos del cliente usando los mismos parámetros del scaler\n",
    "    cliente_normalizado = scaler.transform([[cliente[0], cliente[1], cliente[2], cliente[3]]])\n",
    "    \n",
    "    # Aplicar los pesos\n",
    "    cliente_escalado = [\n",
    "        cliente_normalizado[0][0] * pesos[\"latitud\"],\n",
    "        cliente_normalizado[0][1] * pesos[\"longitud\"],\n",
    "        cliente_normalizado[0][2] * pesos[\"hora\"],\n",
    "        cliente_normalizado[0][3] * pesos[\"dia_semana\"]\n",
    "    ]\n",
    "\n",
    "    # Normalizar y pesar el historial del usuario\n",
    "    historial_usuario_normalizado = historial_usuario.copy()\n",
    "    historial_usuario_normalizado[variables_a_normalizar] = scaler.transform(historial_usuario[variables_a_normalizar])\n",
    "    for columna, peso in pesos.items():\n",
    "        historial_usuario_normalizado[columna] *= peso\n",
    "\n",
    "    # Reentrenar el modelo KNN con el historial del usuario\n",
    "    X_usuario = historial_usuario_normalizado[[\"latitud\", \"longitud\", \"hora\", \"dia_semana\"]]\n",
    "    recomendador.fit(X_usuario)\n",
    "\n",
    "    # Realizar la predicción\n",
    "    distancias, indices = recomendador.kneighbors([cliente_escalado])\n",
    "    comercios_cercanos = historial_usuario.iloc[indices[0]]\n",
    "\n",
    "    # Retornar resultados desnormalizados para mayor interpretabilidad\n",
    "    resultados = comercios_cercanos.copy()\n",
    "    for columna in variables_a_normalizar:\n",
    "        resultados[columna] = scaler.inverse_transform(\n",
    "            [[0] * variables_a_normalizar.index(columna) + [comercios_cercanos[columna].iloc[0]] + [0] * (3 - variables_a_normalizar.index(columna))])[0][variables_a_normalizar.index(columna)]\n",
    "\n",
    "    return resultados[[\"local_nombre\", \"latitud\", \"longitud\", \"hora\", \"dia_semana\"]]\n",
    "\n",
    "# 6. Ejemplo de uso\n",
    "# Datos del cliente (latitud, longitud, hora, día de la semana)\n",
    "cedula_cliente = 8927295120\n",
    "cliente_datos = [-4.0006139264, -79.2049413086, 16, 5]  # Valores originales desnormalizados\n",
    "\n",
    "# Realizar la recomendación\n",
    "recomendaciones = recomendar_por_cedula(cedula_cliente, cliente_datos)\n",
    "\n",
    "# Imprimir los resultados\n",
    "if recomendaciones is not None:\n",
    "    print(\"Comercios más cercanos basados en el historial del usuario (desnormalizados):\")\n",
    "    print(recomendaciones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comercios más cercanos basados en el historial del usuario (normalizados):\n",
      "             local_nombre   latitud  longitud      hora  dia_semana\n",
      "406        Chifa Oriental  0.199854  0.000371  0.956522    1.000000\n",
      "409  Manantial restaurant  0.199852  0.000664  0.869565    1.000000\n",
      "401           Desconocido  0.000002  0.199999  0.652174    1.000000\n",
      "407        Chifa Oriental  0.199854  0.000371  0.695652    0.833333\n",
      "426        Chifa Oriental  0.199854  0.000371  0.434783    0.833333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Practicum2.1\\venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Practicum2.1\\venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Practicum2.1\\venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Practicum2.1\\venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Practicum2.1\\venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Ubicación del cliente y datos contextuales (latitud, longitud, hora, día de la semana)\n",
    "cedula_cliente = 8927295120  # Cédula del cliente\n",
    "cliente_datos = [0.199854, 0.000371,0.956522 , 1.000000]\n",
    "\n",
    "# Realizar la recomendación\n",
    "recomendaciones = recomendar_por_cedula(cedula_cliente, cliente_datos)\n",
    "\n",
    "# Imprimir los resultados\n",
    "if recomendaciones is not None:\n",
    "    print(\"Comercios más cercanos basados en el historial del usuario (normalizados):\")\n",
    "    print(recomendaciones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
